---
title: "Machine_Learning_Bachelor"
author: "Martin Skafte Andersen"
date: "9-12-2020"
output:
  html_notebook:
    code_folding: show
    df_print: paged
    theme: flatly
    toc: yes
    toc_depth: 4
    toc_float:
      collapsed: no
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
---

```{r setup, include=FALSE}
### Generic preamble
rm(list=ls())
Sys.setenv(LANG = "en")
options(scipen = 5)

library(knitr)
knitr::opts_chunk$set(warning=FALSE,
                     message=FALSE,
                     comment=FALSE, 
                     fig.align="center"
                     )
```


```{r}
library(tidyverse)
library(lubridate)
library(Boruta)
library(caret)
library(magrittr)
library(data.table)
library(tidymodels)
library(tidyquant) # For getting stock data
library(ggpubr) #package that helps mixing multiple plots on the same page
```

# Introduction and data

The aim of this assignment is to build a supervised machine learning model and a deep learning model capable of predicting the price of a stock tomorrow. A lot of people has tried to build such a model capable of predicting prices with a high accuracy but without great success. Therefore we did not expect to achieve a model capable of very precise predictions - and if we did we probably would have kept it to ourselves! 

Earlier stock investment was mainly accessible for professional stock brokers but since the release of liberty bonds during the First World War securities became available for the general population. Today it is common for ordinary people to hold stocks and the development in stock prices is therefore of interest to the public.

4 different stocks are chosen for this assignment since stock prices often correlate in some way. Apple is chosen as the main stock of interest and the models are constructed to predict the price of Apple stocks tomorrow. The price is predicted based on the price of Apple stocks today along with the price of 3 other stocks - ITW, IBM and Tesla. Stock prices over 9 years are fetched for each stock.

The data is fetched directly from Yahoo Finance using the `tidyquant` package. (A data set containing these stocks is available at [Github](https://raw.githubusercontent.com/andreasbj77/SDS/main/M3/aktie%20data.csv)).

```{r}
novo <- read.csv("C:/Users/Marti/OneDrive - Aalborg Universitet/6. semester/Bachelor/Data_bachelor/vest.csv")



y <- novo$Adj.Close
y <- (y[-1]-y[-length(y)])/y[-length(y)]

novo = novo[2:nrow(novo),] %>%
  add_column(y)

Stock <- novo
#growth.adj = log(Stock$Adj.Close)-log(lag(Stock$Adj.Close,1))
#Stock = cbind(Stock,growth.adj)
MA10 = rollmean(Stock$y, 2 )
Stock <- Stock[2:nrow(Stock),]
Stock = cbind(Stock,MA10)


Stock %<>%
  add_column(high_minus_low = Stock$High - Stock$Low) %>%
  add_column(open_minus_close = Stock$Close - Stock$Open) %>%
  #add_column(Momentum = Stock$Open-lag(Stock$Close,2)) #%>%
  select(-c(Volume, Open, Close,High, Low,Adj.Close))
```


## Data exploration

To investigate the data and the class of the variables a `glimpse()` of the data is shown. 

```{r}
Stock %>% glimpse()
```

Only the dates and the closing prices are used for prediction. The closing prices are chosen over the other prices since these reflect the next days trading prices best.

```{r}
'novo %<>%
  select(date, close, open, high, low, volume,adjusted, growth.adj,RM.adj,high_minus_low,open_minus_close,open_value ) %>%
  drop_na() '
```

Merging the data sets together and renaming the variables to show where the closing price came from.
```{r}
data <- Stock
```


Tidying up in the environment by removing the old data sets.
```{r}
rm(novo, growth.adj, y, RM.adj, make_binary, ITW, TESLA, aapl, it, iibm, TSLA)
```

The plot below shows the development in each of the stocks in the 9 year period. Some of the stocks seem to be positively correlated and hence move in the same direction eg. Apple and ITW while others seem to be negatively correlated.

Removing unneeded data from the environment.


The variance-covariance matrix is shown below. As expected the correlation between Apple and ITW is strong and positive whereas Apple is negatively correlated with IBM. Overall there seem to be a rather high correlation between Apple and the other stocks, and therefore it is expected that they to some extend are capable of explaining some of the movements in the Apple stock.




For both the supervised machine learning part and for the deep learning part all variable inputs are scaled since it's best practice.
```{r}
#data$Close %<>% scale(center = FALSE, scale = TRUE)
#data$Open %<>% scale(center = FALSE, scale = TRUE)
#data$High %<>% scale(center = FALSE, scale = TRUE)
#data$Low %<>% scale(center = FALSE, scale = TRUE)
#data$Volume %<>% scale(center = FALSE, scale = TRUE)
#data$Adj.Close %<>% scale(center = FALSE, scale = TRUE)
#data$growth.adj %<>% scale(center = FALSE, scale = TRUE)
data$open_minus_close %<>% scale(center = FALSE, scale = TRUE)
#data$Momentum %<>% scale(center = FALSE, scale = TRUE)
data$high_minus_low %<>% scale(center = FALSE, scale = TRUE)
data$MA10 %<>% scale(center = FALSE, scale = TRUE)

#att1 <- attr(data$Adj.Close, 'scaled:scale')




#data$Close %<>% as.numeric()
#data$Open %<>% as.numeric()
#data$High %<>% as.numeric()
#data$Low %<>% as.numeric()
#data$Volume %<>% as.numeric()
# data$Adj.Close %<>% as.numeric()
data$MA10 %<>% as.numeric()
data$open_minus_close %<>% as.numeric()
data$high_minus_low %<>% as.numeric()
#data$Momentum %<>% as.numeric()
#data$growth.adj %<>% as.numeric()


```


# Supervised Machine Learning

## Preprocessing

Since the *data* variable is unneeded for the prediction it is removed from the data set used for the unsupervised machine learning part, *data_sup*. Since the aim is to predict the price of Apple stocks tomorrow a new variable *Appletomorrow* is created which indicates the price of the stock the following day.


```{r}
data_sup <- data %>%
  select(-Date) %>%
  mutate(Y = y %>% lead(1)) %>%
  mutate(Y = ifelse(is.na(Y), lag(Y, 1), Y)) %>%
  relocate(Y)
#For the most recent observation the price of Apple and Appletomorrow are identical
```

## Splitting the data

The data is split into a training set consisting of 80 percent of the data while the remaining 20 percent is test data. 
```{r}
actual <- tail(data_sup[1:552,], 52) 
data_sup <- data_sup[32:500,]
  
data_split_sup <- data_sup %>% initial_time_split(prop = 0.80)

data_train_sup <- data_split_sup %>% training()
data_test_sup <- data_split_sup %>% testing()



  


```

A recipe is defined indicating that *Appletomorrow* is the response variable.
```{r}
data_recipe <- data_train_sup %>%
  recipe(Y  ~.) 


```


## Defining the models

3 models are presented in this section - an elastic net, a random forest and an XgBoost model.

```{r}
set.seed(1337)



#Random forest model
model_rf <- rand_forest(mode = 'regression',
                        trees = 500, 
                        mtry = tune(),
                        min_n = tune()
                        ) %>%
  set_engine('ranger', importance = 'impurity')



 
```

A general workflow along with specialized workflows for each of the models are created.
```{r}
workflow_general <- workflow() %>% #Adding recipe to the general workflow
  add_recipe(data_recipe) 

workflow_rf <- workflow_general %>% #Adding the models to the workflow
  add_model(model_rf)



```

Creating a 3-fold resample with 3 repeats.
```{r}
set.seed(1337)
data_resample <- vfold_cv(data_train_sup, 
                          strata = Y,
                          v = 5 ,
                          repeats = 5)
```

The parameters for each of the models are tuned.
```{r}
set.seed(1337)


#Tuning parameters in the random forest model
tune_rf <-
  tune_grid(
    workflow_rf,
    resamples = data_resample,
    grid = 10
  )
```

Collecting the best parameters for each of the models.
```{r}


best_param_rf <- tune_rf %>% select_best(metric = 'rmse') %>%
      mutate(model = "Random Forest", penalty = "", mixture = "", tree_depth = "", learn_rate="") %>%
  relocate(model)
```

Creating the final workflows where the best parameters are added.
```{r}
#workflow_final_el <- workflow_el %>%
 # finalize_workflow(parameters = best_param_el)

workflow_final_rf <- workflow_rf %>%
  finalize_workflow(parameters = best_param_rf)

#workflow_final_xg <- workflow_xg %>%
 # finalize_workflow(parameters = best_param_xg)
```

Fitting the models on the training data.
```{r}
#fit_el <- workflow_final_el %>%
 # fit(data_train_sup)

fit_rf <- workflow_final_rf %>%
  fit(data_train_sup)

#fit_xg <- workflow_final_xg %>%
 # fit(data_train_sup)
```

Gathering all the predicted and observed prices in *pred_collected* in order to calculate root mean squared error for each of the models.
```{r, warning = FALSE, message = FALSE}
set.seed(1337)
pred_collected <- tibble(
  truth = data_train_sup %>% pull(Y), 
  base = mean(truth), 
  rf = fit_rf %>% predict(new_data = data_train_sup) %>% pull(.pred)) %>% 
  pivot_longer(cols = -truth, 
               names_to = 'model',
               values_to = '.pred')

pred_collected %>%
  group_by(model) %>% 
  rmse(truth = truth, estimate = .pred) %>% 
  select(model, .estimate) %>%  
  arrange(.estimate) 
```
Based on the RMSEs the random forest model makes the most accurate predictions. The model is used for predictions on the test data set.
```{r, warning = FALSE, message = FALSE}
set.seed(1337)
pred_test <- tibble(
  truth = actual %>% pull(Y),
  rf = fit_rf %>% predict(new_data = actual) %>% pull(.pred)) %>% 
  pivot_longer(cols = -truth,
               names_to = 'model',
               values_to = '.pred')

pred_test %>%
  group_by(model) %>% 
  rmse(truth = truth, estimate = .pred) %>% 
  select(model, .estimate) %>%  
  arrange(.estimate)
```

The predictions of the random forest model are plotted below. 
```{r}
pred_test %>%
  ggplot(aes(x = truth, y = .pred)) + 
  geom_abline(lty = 2, color = "gray80", size = 1.5) + 
  geom_point(alpha = 0.4, size = 0.9, color = "#009E73") +
  labs(
    x = "Observed stock price",
    y = "Predicted stock price")
      
```
The grey line indicates where the predicted values are exual to the observed values. The model seems to underpredict for higher stock prices. 

```{r}
library(caret)

boruta_output <- Boruta(Y ~ ., data=na.omit(data_train_sup), doTrace=0,pValue = 0.05)
boruta_signif <- getSelectedAttributes(boruta_output, withTentative = TRUE)
print(boruta_signif) 
roughFixMod <- TentativeRoughFix(boruta_output)
boruta_signif <- getSelectedAttributes(roughFixMod)
print(boruta_signif)
imps <- attStats(roughFixMod)
imps2 = imps[imps$decision != 'Rejected', c('meanImp', 'decision')]
head(imps2[order(-imps2$meanImp), ])  # descending sort
plot(boruta_output, cex.axis=.7, las=2, xlab="", main="Variable Importance") 


```

```{r}
write.table(pred_test,file="VESTAS_results.csv")
```








